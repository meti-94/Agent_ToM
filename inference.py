import os
os.environ['HF_HOME'] = '/mnt/datadisk/Mehdi'

os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
import sys
import argparse
import random
from tqdm import tqdm
import json
import yaml
import torch
from datetime import datetime
import numpy as np
import time
from Search import Search, MyNewSearch
from vllm_run import load_vLLM_model, generate_with_vLLM_model
from utils.gpu_tools import GPUMonitor
from utils.my_node import set_model_args
from local_evaluate import check_answer
from transformer_lens import HookedTransformer
from sae_lens import SAE
import inspect 
from feature_sensitivity_cache import build_cache_paths, load_indices_json
import math

prompt_path = os.path.join(os.getcwd(), "prompts.yaml")
with open(prompt_path, 'r') as f:
    prompts_from_file = yaml.load(f, Loader=yaml.FullLoader)
    f.close()

with open("./data/few_shot_example.json", "r", encoding="utf-8") as json_file:
    few_shot_example = json.load(json_file)
    json_file.close()

def save_config(args):
    os.makedirs(args.output_path, exist_ok=True)
    config_path = os.path.join(args.output_path, 'config.json')
    with open(config_path, 'w') as config_file:
        json.dump(vars(args), config_file, indent=4)

    print(f"Configuration saved to {config_path}")

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def create_output_directory(root, dataset_name, model_name):
    # Get the current time for unique naming
    time_now = datetime.now().strftime("%d_%H%M%S")
    # Create a directory name that includes additional parameters
    dic_name = '{}'.format(time_now)
    # Join the directory name with the root, dataset, and model name
    path = os.path.join(root, dataset_name, model_name, dic_name)
    # Create the directory if it doesn't exist
    os.makedirs(path, exist_ok=True)
    # Print and return the generated path
    print(f"Created directory: {path}")
    return path

def save_tree_to_json(root, filename):
    tree_dict = root.to_dict()
    with open(filename, 'w') as file:
        json.dump(tree_dict, file, indent=4)

def insert_reserved_token(args, string):
    token = args.special_token
    # print('this is special token', token)
    # sys.exit()
    result_list = []
    for _ in range(5):
        if len(string) <= 1:
            result_list.append(string + token)
        else:
            if args.middle_token:
                insert_position = random.randint(1, len(string) - 1)
            elif args.first_token:
                insert_position = 0
            elif args.last_token:
                insert_position = len(string)
            new_string = string[:insert_position] + token + string[insert_position:]
            result_list.append(new_string)
    return result_list


def main(args):
    sae_model=None
    tokenizer, model = load_vLLM_model(args.model_name, seed=args.seed, tensor_parallel_size=args.num_gpu)
    if args.json_file_path is None:
        raise ValueError("json_file_path must be specified")
    with open(args.json_file_path, 'r', encoding="utf-8") as file:
        data = json.load(file)

    for index in tqdm(range(args.begin, args.end)):
        try:
            prompt_entry = data[index]
        except:
            continue
        output_file = os.path.join(args.output_path, f"{index}.json")
        if os.path.exists(output_file):
            print(f"File {output_file} already exists. Skipping this sample.")
            continue

        print("\n")
        answer = prompt_entry.get("answer", "")
        question = prompt_entry.get("question", "")

        question_prompt = ""
        if args.shot == -1:
            question_prompt += f"{question}"
        elif args.shot == 0:
            question_prompt += prompts_from_file[args.dataset+"_zero_shot"] + '\n'
            question_prompt += f"Question:\n{question}\nThought:\nLet’s think step by step. "
        else:
            question_prompt += prompts_from_file[args.dataset+"_zero_shot"] + '\n'
            for i in range(args.shot):
                demo= few_shot_example[args.dataset][i]
                question_prompt += f"Question:\n{demo['Question']}\nThought:\n{demo['Thought']}\nAnswer:\n{demo['Answer']}\n\n"
            question_prompt += f"Question:\n{question}\nThought:\nLet’s think step by step. "

        # print(question_prompt)
        question_prompt_list = insert_reserved_token(args, question_prompt)

        if args.search_algo=='sae':
            # cached = load_indices_json(args.cache_file)
            # top_k_indices = cached.get("top_k_indices", [])
            
            if sae_model==None:
                print(f"[CACHE HIT] Loaded top-k indices from:\n  {args.cache_file}")
                sae_model = HookedTransformer.from_pretrained(args.sae_base_model, device=args.sae_device)
                sae = SAE.from_pretrained(args.release, args.hook_point, device = 'cuda:1')
            Search_agent = MyNewSearch(
                sae_model,
                sae, 
                model=model,
                tokenizer=tokenizer,
                args=args,
                user_prompt = question_prompt_list,
                question=question,
                answer=answer,
                verify_prompt=prompts_from_file[args.dataset.replace("_train", "")],
                num_repeats=args.num_repeats,
                depth_limit=args.depth_limit,
                n_iters=args.num_iteration,
                calc_q=np.mean,
                simulate_strategy='max',
                output_strategy='max_reward',
                disable_tqdm=True,
                save_path =  os.path.join(args.output_path,'{}.json'.format(index))
            )
            Search_agent()

        ###################
        if args.search_algo=='soft_reasoning':
            Search_agent = Search(
                model=model,
                tokenizer=tokenizer,
                args=args,
                user_prompt = question_prompt_list,
                question=question,
                answer=answer,
                verify_prompt=prompts_from_file[args.dataset.replace("_train", "")],
                num_repeats=args.num_repeats,
                depth_limit=args.depth_limit,
                n_iters=args.num_iteration,
                calc_q=np.mean,
                simulate_strategy='max',
                output_strategy='max_reward',
                disable_tqdm=True,
                save_path =  os.path.join(args.output_path,'{}.json'.format(index))
            )
            Search_agent()



if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Text generation script using a pretrained model.")


    # NEW: Search algorithm selector
    parser.add_argument(
        '--search_algo',
        type=str,
        default='sae',
        choices=[
            'soft_reasoning', 'sae'
        ],
        help=(
            'Search algorithm to use for decoding / reasoning. '
        )
    )


    parser.add_argument('--model_name', type=str, default="meta-llama/Meta-Llama-3-8B-Instruct",help='Name or path of the model to use (e.g., meta-llama/Meta-Llama-3-8B-Instruct, mistralai/Mistral-7B-Instruct-v0.3, Qwen/Qwen2-7B-Instruct).')
    parser.add_argument('--dataset', type=str, default="gsm8k", help='Dataset name. Options: gsm8k, gsm_hard, strategyqa, svamp, aime_2024, aime_2025.')
    parser.add_argument('--max_new_tokens', type=int, default=300, help='The maximum numbers of tokens to generate')
    parser.add_argument('--seed', type=int, default=1, help='Seed value for reproducibility')
    parser.add_argument('--random_seed', type=int, default=0, help='Random choose a seed.')
    parser.add_argument('--do_sample', action='store_true', help='Use sampling instead of greedy decoding.')
    parser.add_argument('--use_cache', action='store_true', help='Use cache to speed up decoding.')
    parser.set_defaults(use_cache=True)
    parser.set_defaults(is_gaussian=True)
    parser.add_argument('--stop_threshold', type=float, default=0.01, help='stop_threshold')
    parser.add_argument('--dimention', type=int, default=50, help='projection dimention')
    parser.add_argument('--use_abs', type=int, default=0, help='Top-K sampling parameter.')
    parser.add_argument('--top_p', type=float, default=1.0, help='Nucleus sampling parameter.')
    parser.add_argument('--temperature', type=float, default=1.0, help='Temperature for sampling.')
    parser.add_argument('--top_k', type=int, default=50, help='Top-K sampling parameter.')

    parser.add_argument('--save_path', type=str, default=None, help='Path to save the generated text')
    parser.add_argument('--begin', type=int, default=0, help='Starting index of prompts to process.')
    parser.add_argument('--end', type=int, default=200, help='Ending index of prompts to process.')
    parser.add_argument('--continue_predict', type=int, default=0, help='Flag to continue from the last processed prompt.')
    parser.add_argument('--num_repeats', type=int, default=5, help='Numbw gnjer of times to repeat each prompt generation.')
    parser.add_argument('--num_iteration', type=int, default=1, help='Number of iterations for prompt generation.')
    parser.add_argument('--depth_limit', type=int, default=5, help='Number of iterations for prompt generation.')
    parser.add_argument('--add_best_node', type=int, default=1, help='Flag to include the best node in the output.')
    parser.add_argument('--num_tokens', type=int, default=1, help='Selection method for processing nodes.')
    parser.add_argument('--shot', type=int, default=0, help='Selection method for processing nodes.')
    parser.add_argument('--initial_vector_method', type=int, default= 3, help='Method to initialize vectors for decoding.')

    parser.add_argument('--output_base_path', type=str, default="./output",help='Base path for saving the output data.')
    parser.add_argument('--first_token', type=int, default=0,help='Special token placement at the first position (set to 1 to enable).')
    parser.add_argument('--middle_token', type=int, default=0,help='Special token placement at the middle position (set to 1 to enable).')
    parser.add_argument('--last_token', type=int, default=1,help='Special token placement at the last position (set to 1 to enable).')
    parser.add_argument('--ablation_type', type=int, default=0,help='Ablation experiment configuration (set different values for different setups).')
    parser.add_argument('--for_verifier', type=int, default=1,help='Whether to include verifier information in the reward (1: include, 0: exclude).')
    parser.add_argument('--for_coherence', type=int, default=1,help='Whether to include coherence information in the reward (1: include, 0: exclude).')
    parser.add_argument('--function_method', type=str, default="ei",help='Expected function selection for Bayesian optimization (e.g., "ei", "ucb").')
    parser.add_argument('--ucb_beta', type=float, default=2.0,help='Beta parameter for UCB if Upper Confidence Bound is used as acquisition function.') # 4.5 is good 
    parser.add_argument('--num_gpu', type=int, default=1, help='Number of GPUs to use.')

    parser.add_argument("--sae_base_model", type=str, default="meta-llama/Llama-3.1-8B", help="Base SAE model.")
    parser.add_argument("--release", type=str, default="llama_scope_lxr_32x", help="SAE release used when generating cached indices.")
    parser.add_argument("--hook_point", type=str, default="l28r_32x", help="SAE id/hook point used when generating cached indices.")
    parser.add_argument("--sae_device", type=str, default="cuda:1", help="Device: 'cpu', 'cuda:0', 'mps', etc.")
    parser.add_argument("--steering_scale", type=float, default=4.0, help='The scaling (intensity) factor which is applied to the SAE vectors')
    parser.add_argument("--cache_file", type=str, default="./cache/meta-llama__Llama-3.1-8B/llama_scope_lxr_32x/l28r_32x/763b7c26ea3378a5e7a78903c14f07a7de7edd3c/indices.json", 
                        help="Path to file where associated indices were saved.")
    parser.add_argument("--steering_off", type=bool, default=False, help='This is a boolean parameter who turns steering hook deactivate if it is True')
    parser.add_argument("--generation_temp", type=float, default=0.2, help='This parameter sets the temperature for thought generation.')
    

    args = parser.parse_args()
    set_model_args(args)

    dataset2dir = {
        "gsm8k": "./data/gsm8k.json",
        "gsm_hard": "./data/gsm_hard.json",
        "strategyqa": "./data/strategyqa.json",
        "svamp": "./data/svamp.json",
        "aime_2024": "./data/aime_2024.json",
        "aime_2025": "./data/aime_2025.json",

    }
    if args.random_seed:
        timestamp = int(time.time() * 1000)
        random_seed = timestamp + random.randint(0, 1000)
        random_seed = random_seed % (2 ** 32)
        args.seed = random_seed
    args.json_file_path = dataset2dir[args.dataset]
    set_seed(args.seed)
    if args.continue_predict:
        args.output_path = args.output_base_path
    else:
        args.output_path = create_output_directory(args.output_base_path, args.dataset, args.model_name.split("/")[-1])
    save_config(args)
    main(args)
    check_answer(args.output_path)
    # check_answer('/home/mahdi/Agent_ToM/output/gsm8k/gemma-2-2b/23_060151')
